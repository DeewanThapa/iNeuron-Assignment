{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yQghuA5ix6FZ",
        "outputId": "bc38bd2a-6609-4d11-b3fd-a729f05af2d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tWrite the Python code to implement a single neuron."
      ],
      "metadata": {
        "id": "hZKctLA6yp1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the activation function (ReLU in this case)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Define the single neuron function\n",
        "def single_neuron(inputs, weights, bias):\n",
        "    # Calculate the weighted sum (dot product) of inputs and weights\n",
        "    weighted_sum = np.dot(inputs, weights) + bias\n",
        "    # Apply the activation function\n",
        "    output = relu(weighted_sum)\n",
        "    return output\n",
        "\n",
        "# Example usage\n",
        "# Define input features\n",
        "inputs = np.array([0.5, 0.3, 0.2])  # Example input vector with 3 features\n",
        "\n",
        "# Define weights (same size as inputs)\n",
        "weights = np.array([0.4, 0.7, 0.1])  # Example weights vector\n",
        "\n",
        "# Define bias\n",
        "bias = 0.2  # Example bias term\n",
        "\n",
        "# Calculate the output of the neuron\n",
        "output = single_neuron(inputs, weights, bias)\n",
        "\n",
        "print(f\"Neuron Output: {output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6-_Eo8mzSzq",
        "outputId": "6ad7a3af-4038-498c-91c5-bb1d80d0411d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neuron Output: 0.6300000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tWrite the Python code to implement ReLU."
      ],
      "metadata": {
        "id": "N0AudpnKyp4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Implements the ReLU activation function.\n",
        "\n",
        "    Parameters:\n",
        "    x (numpy array or scalar): The input value or array of values.\n",
        "\n",
        "    Returns:\n",
        "    numpy array or scalar: The output after applying ReLU.\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Example usage\n",
        "# For a single value\n",
        "input_value = -3.5\n",
        "output_value = relu(input_value)\n",
        "print(f\"ReLU({input_value}) = {output_value}\")\n",
        "\n",
        "# For an array of values\n",
        "input_array = np.array([-2, -1, 0, 1, 2, 3])\n",
        "output_array = relu(input_array)\n",
        "print(f\"ReLU({input_array}) = {output_array}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPkWvhW6zc2x",
        "outputId": "17f32dd7-988d-4fcc-fe96-7f112c079102"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU(-3.5) = 0.0\n",
            "ReLU([-2 -1  0  1  2  3]) = [0 0 0 1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tWrite the Python code for a dense layer in terms of matrix multiplication."
      ],
      "metadata": {
        "id": "IR3BIuVzyp5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    ReLU activation function.\n",
        "\n",
        "    Parameters:\n",
        "    x (numpy array): The input array.\n",
        "\n",
        "    Returns:\n",
        "    numpy array: The output array after applying ReLU.\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def dense_layer(inputs, weights, bias, activation_function=None):\n",
        "    \"\"\"\n",
        "    Implements a dense layer using matrix multiplication.\n",
        "\n",
        "    Parameters:\n",
        "    inputs (numpy array): The input data, shape (n_samples, n_features).\n",
        "    weights (numpy array): The weight matrix, shape (n_features, n_neurons).\n",
        "    bias (numpy array): The bias vector, shape (n_neurons,).\n",
        "    activation_function (function, optional): The activation function to apply.\n",
        "\n",
        "    Returns:\n",
        "    numpy array: The output of the dense layer after applying the activation function.\n",
        "    \"\"\"\n",
        "    # Perform matrix multiplication and add the bias\n",
        "    z = np.dot(inputs, weights) + bias\n",
        "\n",
        "    # Apply activation function if specified\n",
        "    if activation_function is not None:\n",
        "        return activation_function(z)\n",
        "    else:\n",
        "        return z\n",
        "\n",
        "# Example usage\n",
        "# Define input features (shape: n_samples x n_features)\n",
        "inputs = np.array([[0.5, 0.3, 0.2],\n",
        "                   [0.8, 0.1, 0.7]])  # Example input with 2 samples and 3 features\n",
        "\n",
        "# Define weights (shape: n_features x n_neurons)\n",
        "weights = np.array([[0.4, 0.7],\n",
        "                    [0.1, 0.9],\n",
        "                    [0.3, 0.5]])  # Example weights for a layer with 3 features and 2 neurons\n",
        "\n",
        "# Define bias (shape: n_neurons,)\n",
        "bias = np.array([0.2, 0.3])  # Example bias for 2 neurons\n",
        "\n",
        "# Calculate the output of the dense layer with ReLU activation\n",
        "output = dense_layer(inputs, weights, bias, activation_function=relu)\n",
        "\n",
        "print(\"Output of the dense layer:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ahHckOjzlsa",
        "outputId": "d31b58c4-88f0-45db-a315-acd0ab0a9c5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the dense layer:\n",
            "[[0.49 1.02]\n",
            " [0.74 1.3 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tWrite the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python)."
      ],
      "metadata": {
        "id": "HYy-gM1Nyp70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ReLU activation function\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    ReLU activation function.\n",
        "\n",
        "    Parameters:\n",
        "    x (float): The input value.\n",
        "\n",
        "    Returns:\n",
        "    float: The output after applying ReLU.\n",
        "    \"\"\"\n",
        "    return max(0, x)\n",
        "\n",
        "# Function to compute dot product of two vectors\n",
        "def dot_product(vector1, vector2):\n",
        "    \"\"\"\n",
        "    Compute the dot product of two vectors.\n",
        "\n",
        "    Parameters:\n",
        "    vector1 (list of float): The first vector.\n",
        "    vector2 (list of float): The second vector.\n",
        "\n",
        "    Returns:\n",
        "    float: The dot product of the two vectors.\n",
        "    \"\"\"\n",
        "    return sum(v1 * v2 for v1, v2 in zip(vector1, vector2))\n",
        "\n",
        "# Define the dense layer function\n",
        "def dense_layer(inputs, weights, bias, activation_function=None):\n",
        "    \"\"\"\n",
        "    Implements a dense layer using plain Python.\n",
        "\n",
        "    Parameters:\n",
        "    inputs (list of float): The input data.\n",
        "    weights (list of list of float): The weight matrix.\n",
        "    bias (list of float): The bias vector.\n",
        "    activation_function (function, optional): The activation function to apply.\n",
        "\n",
        "    Returns:\n",
        "    list of float: The output of the dense layer after applying the activation function.\n",
        "    \"\"\"\n",
        "    outputs = []\n",
        "    for neuron_weights, neuron_bias in zip(weights, bias):\n",
        "        # Calculate the weighted sum for each neuron\n",
        "        weighted_sum = dot_product(inputs, neuron_weights) + neuron_bias\n",
        "        # Apply the activation function if provided\n",
        "        if activation_function is not None:\n",
        "            output = activation_function(weighted_sum)\n",
        "        else:\n",
        "            output = weighted_sum\n",
        "        outputs.append(output)\n",
        "    return outputs\n",
        "\n",
        "# Example usage\n",
        "# Define input features\n",
        "inputs = [0.5, 0.3, 0.2]  # Example input vector with 3 features\n",
        "\n",
        "# Define weights (list of lists, where each list represents the weights for one neuron)\n",
        "weights = [\n",
        "    [0.4, 0.7, 0.1],  # Weights for neuron 1\n",
        "    [0.8, 0.5, 0.3]   # Weights for neuron 2\n",
        "]\n",
        "\n",
        "# Define bias (one value per neuron)\n",
        "bias = [0.2, 0.3]  # Bias for 2 neurons\n",
        "\n",
        "# Calculate the output of the dense layer with ReLU activation\n",
        "output = dense_layer(inputs, weights, bias, activation_function=relu)\n",
        "\n",
        "print(\"Output of the dense layer:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4H4u8eLztFp",
        "outputId": "d0f3721a-dbe4-4713-d81d-d3fd5730772d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the dense layer: [0.6300000000000001, 0.9100000000000001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tWhat is the “hidden size” of a layer?"
      ],
      "metadata": {
        "id": "Qd8iTKnNyp-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A5. In the context of neural networks, the term \"hidden size\" typically refers to the number of neurons (or units) in a hidden layer of the network. Here's a more detailed explanation:\n",
        "\n",
        "Hidden Size of a Layer\n",
        "Hidden Layer:\n",
        "\n",
        "A hidden layer is any layer in a neural network that is not an input layer or an output layer. These layers are \"hidden\" because they are not directly exposed to the input data or the final output. Instead, they sit between the input and output layers, processing intermediate representations of the data.\n",
        "Hidden layers enable the network to learn complex patterns and representations by transforming the input data through multiple stages.\n",
        "Hidden Size:\n",
        "\n",
        "The hidden size refers to the number of neurons present in a hidden layer. It is a crucial hyperparameter in designing neural networks, as it determines the capacity of the network to learn features from the data.\n",
        "For example, if a hidden layer has a hidden size of 64, it means there are 64 neurons in that layer. Each neuron will take inputs from the previous layer and output to the next layer, contributing to the overall transformation of the input data as it passes through the network.\n",
        "Importance of Hidden Size\n",
        "Capacity to Learn: A larger hidden size can increase the capacity of the network to learn complex patterns from the data because more neurons mean more weights and biases that the model can adjust during training.\n",
        "\n",
        "Computational Cost: Increasing the hidden size also increases the computational cost and memory requirements. Each additional neuron requires its own set of weights, biases, and computations, which can lead to longer training times and more resource consumption.\n",
        "\n",
        "Overfitting and Underfitting:\n",
        "\n",
        "If the hidden size is too large relative to the amount of training data, the network might overfit, learning specific details and noise from the training data rather than general patterns. Overfitting can cause poor performance on unseen data.\n",
        "If the hidden size is too small, the network might underfit, failing to capture important patterns and relationships in the data.\n",
        "Choosing Hidden Size\n",
        "There is no one-size-fits-all rule for choosing the hidden size. It often depends on factors like the complexity of the task, the amount of training data available, and empirical testing.\n",
        "Common strategies include:\n",
        "Starting Small: Begin with a smaller hidden size and increase if the network is underfitting.\n",
        "Incremental Testing: Experiment with different sizes and evaluate the performance using validation data to find a suitable balance.\n",
        "Guidelines from Literature: For well-studied problems, existing research can provide insights into typical hidden sizes used.\n",
        "Example\n",
        "Consider a neural network for image classification:\n",
        "\n",
        "Input Layer: Takes a flattened 28x28 pixel grayscale image, so it has 784 input neurons.\n",
        "First Hidden Layer: Has a hidden size of 128 neurons. This means there are 128 neurons in this layer, each connected to the 784 input neurons, with their own set of weights and biases.\n",
        "Second Hidden Layer: Has a hidden size of 64 neurons, taking input from the 128 neurons of the first hidden layer.\n",
        "Output Layer: Outputs 10 classes, corresponding to digits 0-9, so it has 10 neurons."
      ],
      "metadata": {
        "id": "66V4jb6byqOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.\tWhat does the t method do in PyTorch?"
      ],
      "metadata": {
        "id": "jXEUWdUCyqQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A3. In PyTorch, the t() method is used to transpose a tensor. Transposition is the process of interchanging the dimensions of a tensor. This operation is commonly used in various neural network operations, such as matrix multiplication, reshaping, and permuting dimensions.\n",
        "\n",
        "Key points about the t() method:\n",
        "\n",
        "Transposes dimensions: It swaps the first and second dimensions of a tensor, effectively rotating it by 90 degrees.\n",
        "In-place operation: By default, the t() method operates in-place, modifying the original tensor. You can create a copy of the tensor before applying t() if you want to preserve the original tensor.\n",
        "Works on tensors of any dimension: The t() method can be used on tensors of any rank (number of dimensions).\n",
        "Example:\n",
        "\n",
        "Python\n",
        "import torch\n",
        "\n",
        "# Create a 2D tensor\n",
        "tensor = torch.randn(3, 4)\n",
        "\n",
        "# Transpose the tensor\n",
        "transposed_tensor = tensor.t()\n",
        "\n",
        "print(\"Original tensor:\")\n",
        "print(tensor)\n",
        "\n",
        "print(\"Transposed tensor:\")\n",
        "print(transposed_tensor)\n",
        "Use code with caution.\n",
        "\n",
        "In this example, a 3x4 tensor is created. The t() method is then used to transpose it, resulting in a 4x3 tensor."
      ],
      "metadata": {
        "id": "-Qjs79RkyqSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.\tWhy is matrix multiplication written in plain Python very slow?"
      ],
      "metadata": {
        "id": "d8OWf4H5yqUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A7. Matrix multiplication written in plain Python can be very slow due to several inherent limitations related to how Python handles computations, memory, and data structures. Here are the main reasons:\n",
        "\n",
        "1. Interpreted Language Overhead\n",
        "Python is an interpreted language, meaning each line of code is executed by the Python interpreter rather than being compiled directly into machine code. This adds overhead because the interpreter must translate each operation at runtime, which is inherently slower than executing pre-compiled code.\n",
        "2. Lack of Native Numerical Support\n",
        "Python's built-in data structures, like lists, are not optimized for numerical computations. Lists are general-purpose and can store different types of data, which introduces flexibility but also inefficiency. Each element in a Python list is a reference to a Python object, which incurs additional overhead compared to raw numerical data types used in low-level languages like C or Fortran.\n",
        "3. Inefficient Memory Access Patterns\n",
        "Matrix multiplication involves accessing and computing multiple rows and columns in a structured manner. Python lists do not store elements contiguously in memory (as arrays do in languages like C), leading to poor cache performance. This inefficiency means more time is spent fetching data from memory, causing cache misses and slowing down computations.\n",
        "4. No Low-Level Optimization\n",
        "Plain Python loops for matrix multiplication lack optimizations that are typical in numerical libraries or lower-level languages. These optimizations include:\n",
        "Loop unrolling: A technique that reduces the overhead of loop control.\n",
        "Vectorization: Using Single Instruction, Multiple Data (SIMD) instructions that allow a single operation to be performed on multiple data points simultaneously.\n",
        "Parallelization: Taking advantage of multi-core processors to perform multiple calculations at once.\n",
        "Python's default interpreter does not provide these optimizations automatically.\n",
        "5. Global Interpreter Lock (GIL)\n",
        "Python's GIL prevents multiple native threads from executing Python bytecodes simultaneously. This lock limits the ability to use multiple cores for parallel execution, which could otherwise speed up operations like matrix multiplication.\n",
        "6. Dynamic Typing\n",
        "Python's dynamic typing requires runtime type checks, which introduce additional overhead. In numerical computations, knowing the type of each variable in advance allows for more optimized machine code. The lack of type specificity in plain Python means that operations are slower because Python must handle various possible types dynamically.\n",
        "7. Inefficient Looping Constructs\n",
        "Matrix multiplication typically involves three nested loops: one for iterating over the rows of the first matrix, one for iterating over the columns of the second matrix, and one for performing the dot product. This results in an O(n^3) time complexity. In Python, each of these loops involves multiple overheads related to loop control, object referencing, and data fetching."
      ],
      "metadata": {
        "id": "0SUsnlFUyqYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.\tIn matmul, why is ac==br?"
      ],
      "metadata": {
        "id": "s-EwpjfGyqaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A8. Understanding matmul and the ac==br Condition\n",
        "\n",
        "In matrix multiplication, denoted as matmul in many programming languages, the dimensions of the matrices must satisfy a specific condition: the number of columns in the first matrix must equal the number of rows in the second matrix. This condition is often expressed as ac==br, where:\n",
        "\n",
        "a is the number of rows in the first matrix.\n",
        "c is the number of columns in the first matrix.\n",
        "b is the number of rows in the second matrix.\n",
        "r is the number of columns in the second matrix.\n",
        "Why is ac==br necessary?\n",
        "\n",
        "The ac==br condition ensures that the inner dimensions of the matrices are compatible for element-wise multiplication. When multiplying two matrices, each element in the resulting matrix is calculated by taking the dot product of a row from the first matrix and a column from the second matrix. For this dot product to be defined, the length of the row and the length of the column must be the same.\n",
        "\n",
        "Visual Example:\n",
        "\n",
        "Consider two matrices:\n",
        "\n",
        "A = [[1, 2, 3],\n",
        "     [4, 5, 6]]\n",
        "B = [[7, 8],\n",
        "     [9, 10],\n",
        "     [11, 12]]\n",
        "Here:\n",
        "\n",
        "a = 2 (rows in A)\n",
        "c = 3 (columns in A)\n",
        "b = 3 (rows in B)\n",
        "r = 2 (columns in B)\n",
        "Since ac (2*3) == br (3*2), matrix multiplication is possible. The resulting matrix will have dimensions a x r (2 x 2).\n",
        "\n",
        "In summary, the ac==br condition is essential for matrix multiplication because it ensures that the inner dimensions of the matrices are compatible for element-wise multiplication, allowing the dot product to be calculated correctly.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9FVfsBI_yqbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.\tIn Jupyter Notebook, how do you measure the time taken for a single cell to execute?"
      ],
      "metadata": {
        "id": "VsHtzONs0QKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A9. Measuring Execution Time in Jupyter Notebook\n",
        "\n",
        "There are several ways to measure the execution time of a single cell in Jupyter Notebook:\n",
        "\n",
        "1. Using the built-in %%timeit magic command:\n",
        "\n",
        "Place this magic command at the beginning of the cell.\n",
        "It will run the cell multiple times and report the average execution time.\n",
        "Python\n",
        "%%timeit\n",
        "# Your code here\n",
        "Use code with caution.\n",
        "\n",
        "2. Using the time module:\n",
        "\n",
        "Import the time module.\n",
        "Record the start time before executing the code.\n",
        "Record the end time after the code finishes.\n",
        "Calculate the elapsed time.\n",
        "Python\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "# Your code here\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
        "Use code with caution.\n",
        "\n",
        "3. Using the ipython_time extension:\n",
        "\n",
        "Install the extension using pip install ipython_time.\n",
        "Load the extension in your notebook.\n",
        "The execution time will be displayed in the cell's output.\n",
        "Python\n",
        "%load_ext ipython_time\n",
        "# Your code here\n",
        "Use code with caution.\n",
        "\n",
        "4. Using a profiler:\n",
        "\n",
        "Use a profiler like cProfile or line_profiler to get a detailed breakdown of execution time for different parts of your code.\n",
        "Python\n",
        "%load_ext line_profiler\n",
        "%lprun -f your_function your_function()\n",
        "Use code with caution.\n",
        "\n",
        "Choose the method that best suits your needs based on the level of detail you require and your preference for how the results are displayed."
      ],
      "metadata": {
        "id": "MeafsrY60SgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.\tWhat is elementwise arithmetic?"
      ],
      "metadata": {
        "id": "V1_Prjbl0VsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A10. Elementwise Arithmetic\n",
        "\n",
        "Elementwise arithmetic is a type of operation performed on arrays or matrices where each corresponding element of the two arrays or matrices is combined using a specific arithmetic operation. This operation can be addition, subtraction, multiplication, or division.\n",
        "\n",
        "Example:\n",
        "\n",
        "A = [1, 2, 3]\n",
        "B = [4, 5, 6]\n",
        "\n",
        "# Elementwise addition:\n",
        "C = A + B = [1+4, 2+5, 3+6] = [5, 7, 9]\n",
        "In this example, the corresponding elements of arrays A and B are added together to create a new array C.\n",
        "\n",
        "Key points to remember:\n",
        "\n",
        "The two arrays or matrices must have the same dimensions for elementwise arithmetic to be performed.\n",
        "The resulting array or matrix will have the same dimensions as the input arrays or matrices.\n",
        "Elementwise arithmetic is a common operation in various fields, including linear algebra, machine learning, and data analysis.\n",
        "Other examples of elementwise arithmetic:\n",
        "\n",
        "Elementwise subtraction: C = A - B\n",
        "Elementwise multiplication: C = A * B\n",
        "Elementwise division: C = A / B (only if all elements in B are non-zero)\n",
        "By understanding elementwise arithmetic, you can effectively perform operations on arrays and matrices in various programming languages and applications."
      ],
      "metadata": {
        "id": "lTluBQPT0Y0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.\tWrite the PyTorch code to test whether every element of a is greater than the corresponding element of b."
      ],
      "metadata": {
        "id": "2c8lqgoj0Z6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "def elementwise_greater(a, b):\n",
        "  \"\"\"\n",
        "  Checks if every element of tensor a is greater than the corresponding element of tensor b.\n",
        "\n",
        "  Args:\n",
        "    a: A PyTorch tensor.\n",
        "    b: A PyTorch tensor of the same shape as a.\n",
        "\n",
        "  Returns:\n",
        "    A boolean indicating whether every element of a is greater than the corresponding element of b.\n",
        "  \"\"\"\n",
        "\n",
        "  return (a > b).all()\n",
        "\n",
        "# Example usage:\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([0, 1, 2])\n",
        "\n",
        "result = elementwise_greater(a, b)\n",
        "print(result)  # Output: True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pMQqtwQx-bo",
        "outputId": "fda129cb-bde1-4fad-ff7f-abee294827d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.\tWhat is a rank-0 tensor? How do you convert it to a plain Python data type?"
      ],
      "metadata": {
        "id": "1ZT1I2e_0jaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A12. Rank-0 Tensor\n",
        "\n",
        "A rank-0 tensor is essentially a scalar value. It's the simplest type of tensor in PyTorch, representing a single element or a single number. Think of it as a tensor with no dimensions.\n",
        "\n",
        "Converting to Plain Python Data Type\n",
        "\n",
        "To convert a rank-0 tensor to a plain Python data type, you can use the item() method. This method extracts the underlying scalar value from the tensor and returns it as a Python data type (e.g., int, float).\n",
        "\n",
        "Example:\n",
        "\n",
        "Python\n",
        "import torch\n",
        "\n",
        "# Create a rank-0 tensor\n",
        "scalar_tensor = torch.tensor(5)\n",
        "\n",
        "# Convert to a plain Python integer\n",
        "scalar_value = scalar_tensor.item()\n",
        "\n",
        "print(scalar_value)  # Output: 5\n",
        "Use code with caution.\n",
        "\n",
        "In this example, scalar_tensor is a rank-0 tensor containing the value 5. The item() method extracts this value and stores it in the scalar_value variable, which is now a Python integer.\n",
        "\n",
        "Key points:\n",
        "\n",
        "A rank-0 tensor is a scalar value.\n",
        "The item() method is used to convert a rank-0 tensor to a plain Python data type.\n",
        "The returned data type will depend on the type of the tensor's elements."
      ],
      "metadata": {
        "id": "f13GHjpL03sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.\tHow does elementwise arithmetic help us speed up matmul?"
      ],
      "metadata": {
        "id": "BITjy8aS04dC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elementwise Arithmetic and Speeding Up matmul\n",
        "\n",
        "While elementwise arithmetic itself doesn't directly speed up matrix multiplication (matmul), it's often a fundamental operation used within optimized matmul implementations. Let's break down how:\n",
        "\n",
        "1. BLAS Libraries:\n",
        "\n",
        "Optimized implementations: BLAS (Basic Linear Algebra Subprograms) libraries, like BLAS, ATLAS, or OpenBLAS, provide highly optimized implementations for various linear algebra operations, including matmul.\n",
        "Elementwise operations: These libraries often use elementwise arithmetic operations as building blocks for matmul. For instance, they might break down matrix multiplication into a series of elementwise multiplications and additions.\n",
        "Hardware-specific optimizations: BLAS libraries are often optimized for specific hardware architectures (e.g., CPUs, GPUs), leveraging SIMD instructions or other techniques to improve performance.\n",
        "2. GPU Acceleration:\n",
        "\n",
        "Parallel processing: GPUs are designed for parallel processing, making them well-suited for elementwise operations on large matrices.\n",
        "CUDA and cuBLAS: Frameworks like CUDA (Compute Unified Device Architecture) and cuBLAS (CUDA Basic Linear Algebra Subprograms) provide highly optimized implementations of matmul and other linear algebra operations for GPUs.\n",
        "Elementwise operations on GPUs: These implementations often leverage elementwise arithmetic operations on GPU cores to achieve significant speedups.\n",
        "3. Compiler Optimizations:\n",
        "\n",
        "Loop unrolling: Compilers can optimize loops involving elementwise arithmetic by unrolling them, which can reduce overhead and improve performance.\n",
        "SIMD instructions: Compilers can also generate SIMD instructions to perform multiple elementwise operations simultaneously, further improving performance."
      ],
      "metadata": {
        "id": "q5rNq9jp04fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are the broadcasting rules?"
      ],
      "metadata": {
        "id": "z90O0VPz1ecW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A14. Broadcasting Rules\n",
        "\n",
        "Broadcasting is a mechanism in NumPy that allows arrays of different shapes to be combined elementwise. It follows a set of rules to ensure compatibility:\n",
        "\n",
        "Shape Compatibility:\n",
        "\n",
        "The arrays must have the same number of dimensions.\n",
        "The shape of one array can be extended to match the shape of the other array if one of the following conditions holds:\n",
        "The shape of the smaller array is 1.\n",
        "The corresponding dimension of the smaller array is equal to the corresponding dimension of the larger array.\n",
        "Extension:\n",
        "\n",
        "The smaller array is \"stretched\" or \"tiled\" to match the larger array's shape.\n",
        "This extension is done along the dimensions where the smaller array's shape is 1."
      ],
      "metadata": {
        "id": "wB5GiPIL04jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.\tWhat is expand_as? Show an example of how it can be used to match the results of broadcasting."
      ],
      "metadata": {
        "id": "wV_7rKgI04ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, the expand_as method is used to expand the dimensions of a tensor to match the shape of another tensor. This is particularly useful when you want to perform operations between tensors of different shapes, such as addition or multiplication, by ensuring that they are compatible through broadcasting.\n",
        "\n",
        "What is expand_as?\n",
        "Function: expand_as expands the dimensions of a tensor to match the shape of the specified tensor.\n",
        "Usage: It is typically used to adjust the shape of a tensor so that it can be broadcasted to the shape of another tensor.\n",
        "Example of Using expand_as\n",
        "Let's illustrate how expand_as can be used with a practical example. Suppose you have a tensor A and you want to perform an operation with another tensor B, but their shapes are not directly compatible. You can use expand_as to adjust A's shape to match B's shape.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import torch\n",
        "\n",
        "# Define two tensors with different shapes\n",
        "A = torch.tensor([1, 2, 3])  # Shape: (3,)\n",
        "B = torch.tensor([[10, 20, 30], [40, 50, 60]])  # Shape: (2, 3)\n",
        "\n",
        "# Expand tensor A to match the shape of tensor B\n",
        "A_expanded = A.expand_as(B)\n",
        "\n",
        "# Perform an element-wise addition\n",
        "result = A_expanded + B\n",
        "\n",
        "print(\"Tensor A:\", A)\n",
        "print(\"Tensor B:\", B)\n",
        "print(\"Expanded A:\", A_expanded)\n",
        "print(\"Result of A_expanded + B:\", result)\n",
        "Explanation:\n",
        "Define Tensors:\n",
        "\n",
        "A is a 1D tensor with shape (3,).\n",
        "B is a 2D tensor with shape (2, 3).\n",
        "Expand Tensor A:\n",
        "\n",
        "A.expand_as(B) expands A to match the shape of B which is (2, 3).\n",
        "The expanded tensor A_expanded now has the same shape as B, where each row of A is replicated to match the number of rows in B.\n",
        "Element-wise Addition:\n",
        "\n",
        "result = A_expanded + B performs element-wise addition of A_expanded and B.\n",
        "Output:\n",
        "lua\n",
        "Copy code\n",
        "Tensor A: tensor([1, 2, 3])\n",
        "Tensor B: tensor([[10, 20, 30],\n",
        "                  [40, 50, 60]])\n",
        "Expanded A: tensor([[1, 2, 3],\n",
        "                     [1, 2, 3]])\n",
        "Result of A_expanded + B: tensor([[11, 22, 33],\n",
        "                                   [41, 52, 63]])"
      ],
      "metadata": {
        "id": "NTizake613K8"
      }
    }
  ]
}