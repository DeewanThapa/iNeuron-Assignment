{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c39198c-fd57-40ee-bf9a-e029211ae3f7",
   "metadata": {},
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366c321-b73e-49f1-b775-ba5a43641bb7",
   "metadata": {},
   "source": [
    "A1. Feature Engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, leading to improved model performance. It involves creating new features, modifying existing ones, or selecting the most relevant features from a dataset.\n",
    "\n",
    "How Feature Engineering Works\n",
    "Understanding the Problem Domain:\n",
    "\n",
    "Objective: Gain insight into the problem you're solving and how features relate to the target variable.\n",
    "Approach: Collaborate with domain experts to identify which features could be important and why. This understanding guides the entire feature engineering process.\n",
    "Data Exploration and Analysis:\n",
    "\n",
    "Objective: Explore and analyze the raw data to understand its structure, distribution, and relationships.\n",
    "Approach: Perform exploratory data analysis (EDA) using statistical summaries, visualizations (e.g., histograms, scatter plots), and correlation matrices. This helps identify patterns, anomalies, and relationships.\n",
    "Feature Creation:\n",
    "\n",
    "Objective: Generate new features that might enhance the model's ability to learn.\n",
    "Approach: Use domain knowledge, mathematical operations, or data aggregation to create features. For example, combining existing features to create interaction terms or generating time-based features from date columns.\n",
    "Feature Transformation:\n",
    "\n",
    "Objective: Modify or scale existing features to improve their usability in the model.\n",
    "Approach: Apply techniques such as normalization, standardization, log transformations, or polynomial features to adjust feature scales or address skewness.\n",
    "Feature Selection:\n",
    "\n",
    "Objective: Identify and retain the most relevant features for the model while removing redundant or irrelevant ones.\n",
    "Approach: Use statistical tests, model-based methods, or iterative approaches to evaluate feature importance and select a subset of features.\n",
    "Feature Encoding:\n",
    "\n",
    "Objective: Convert categorical or non-numeric data into numerical formats that models can work with.\n",
    "Approach: Apply encoding techniques such as one-hot encoding, label encoding, or embeddings to represent categorical variables numerically.\n",
    "Feature Reduction:\n",
    "\n",
    "Objective: Reduce the number of features to simplify the model and improve performance.\n",
    "Approach: Use dimensionality reduction techniques to project data into a lower-dimensional space while preserving important information.\n",
    "Various Aspects of Feature Engineering in Depth\n",
    "1. Feature Creation\n",
    "Aggregation:\n",
    "\n",
    "Description: Summarize information from multiple features to create new features.\n",
    "Examples:\n",
    "Mean: Average of a group of features (e.g., average transaction amount).\n",
    "Sum: Total amount of transactions in a dataset.\n",
    "Interaction Terms:\n",
    "\n",
    "Description: Create new features by combining existing ones to capture interactions.\n",
    "Examples:\n",
    "Product: Product of two features (e.g., height * weight).\n",
    "Ratio: Ratio of two features (e.g., income / expenditure).\n",
    "Domain-Specific Features:\n",
    "\n",
    "Description: Features based on domain expertise that provide additional insight.\n",
    "Examples:\n",
    "Time-Based Features: Extract day of the week, month, or year from date fields.\n",
    "Text Features: Create features based on text data, such as word counts or sentiment scores.\n",
    "2. Feature Transformation\n",
    "Scaling:\n",
    "\n",
    "Description: Adjust the range of feature values to a common scale.\n",
    "Examples:\n",
    "Standardization: Center features around zero with unit variance (z-score normalization).\n",
    "Normalization: Scale features to a specific range, such as [0, 1].\n",
    "Log Transformation:\n",
    "\n",
    "Description: Apply a logarithmic transformation to handle skewed distributions and reduce the impact of outliers.\n",
    "Examples:\n",
    "Log(x + 1): Used to compress skewed data.\n",
    "Polynomial Features:\n",
    "\n",
    "Description: Create features that are polynomial combinations of existing features.\n",
    "Examples:\n",
    "Quadratic Terms: Add squared terms (e.g., x²).\n",
    "Interaction Terms: Add interaction terms of features (e.g., x1 * x2).\n",
    "3. Feature Selection\n",
    "Filter Methods:\n",
    "\n",
    "Description: Evaluate features based on statistical properties without involving a learning model.\n",
    "Examples:\n",
    "Chi-Square Test: Measures independence between categorical features and the target.\n",
    "Correlation Coefficients: Measures linear relationship between numeric features and the target.\n",
    "Wrapper Methods:\n",
    "\n",
    "Description: Evaluate feature subsets by training and validating a model using those features.\n",
    "Examples:\n",
    "Forward Selection: Iteratively adds features and evaluates performance.\n",
    "Backward Elimination: Iteratively removes features and evaluates performance.\n",
    "Embedded Methods:\n",
    "\n",
    "Description: Perform feature selection as part of the model training process.\n",
    "Examples:\n",
    "Lasso Regression: Uses L1 regularization to shrink some coefficients to zero.\n",
    "Tree-Based Methods: Feature importance scores from models like Random Forests.\n",
    "4. Feature Encoding\n",
    "One-Hot Encoding:\n",
    "\n",
    "Description: Convert categorical variables into binary columns for each category.\n",
    "Example: For a feature with categories \"Red\", \"Blue\", \"Green\", create three binary columns.\n",
    "Label Encoding:\n",
    "\n",
    "Description: Assign integer values to categories.\n",
    "Example: Encode \"Red\" as 1, \"Blue\" as 2, and \"Green\" as 3.\n",
    "Embeddings:\n",
    "\n",
    "Description: Use dense vector representations for categorical features, especially useful for high-cardinality categories.\n",
    "Example: Word embeddings like Word2Vec or GloVe for text data.\n",
    "5. Feature Reduction\n",
    "Dimensionality Reduction:\n",
    "\n",
    "Description: Reduce the number of features while retaining as much information as possible.\n",
    "Examples:\n",
    "Principal Component Analysis (PCA): Projects data into a lower-dimensional space based on variance.\n",
    "t-SNE: Non-linear dimensionality reduction for visualization purposes.\n",
    "Feature Extraction:\n",
    "\n",
    "Description: Transform features into a lower-dimensional space.\n",
    "Examples:\n",
    "Singular Value Decomposition (SVD): Factorizes matrices into components for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a36826-49a5-4632-9bd1-60dea37feeeb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c09e3d40-1ee4-4407-9667-63acaefe5c37",
   "metadata": {},
   "source": [
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe85dd2-1328-495f-a08c-16f4b87b97c1",
   "metadata": {},
   "source": [
    "A2. Feature Selection is the process of identifying and selecting a subset of the most relevant features from the original set of features in a dataset. The goal is to improve the performance of a machine learning model by focusing on the most important features, reducing dimensionality, and eliminating irrelevant or redundant features.\n",
    "\n",
    "Aim of Feature Selection\n",
    "Improve Model Performance:\n",
    "\n",
    "Accuracy: Selecting the most relevant features can enhance the accuracy of the model by focusing on the most informative variables.\n",
    "Overfitting: Reducing the number of features helps to prevent overfitting, where the model performs well on training data but poorly on unseen data.\n",
    "Reduce Computational Cost:\n",
    "\n",
    "Efficiency: Fewer features mean less computational resources and faster training and inference times.\n",
    "Storage: Reducing the number of features decreases the amount of storage required for the dataset.\n",
    "Enhance Model Interpretability:\n",
    "\n",
    "Simplicity: Models with fewer features are easier to interpret and understand, which is crucial for explaining predictions to stakeholders.\n",
    "Handle Redundancy:\n",
    "\n",
    "Reduction: Eliminate redundant features that provide similar information, leading to a more efficient and streamlined model.\n",
    "Methods of Feature Selection\n",
    "Feature selection methods can be broadly classified into three categories: Filter Methods, Wrapper Methods, and Embedded Methods. Each approach has its own advantages and use cases.\n",
    "\n",
    "1. Filter Methods\n",
    "Definition: Filter methods evaluate the relevance of features based on statistical measures or metrics, independently of any machine learning model.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "Independence: Evaluate features based on their intrinsic properties without considering interactions with other features.\n",
    "Speed: Typically faster as they don’t involve training a model.\n",
    "Common Techniques:\n",
    "\n",
    "Correlation Coefficient: Measures the linear relationship between features and the target variable. Features with low correlation to the target or high correlation with other features may be removed.\n",
    "Chi-Square Test: Assesses the independence between categorical features and the target variable. Used primarily for categorical data.\n",
    "Mutual Information: Measures the amount of information obtained about one feature by observing another. Higher mutual information indicates a more relevant feature.\n",
    "Advantages:\n",
    "\n",
    "Simple and computationally efficient.\n",
    "Does not require model training.\n",
    "Disadvantages:\n",
    "\n",
    "May not capture interactions between features and the target variable.\n",
    "2. Wrapper Methods\n",
    "Definition: Wrapper methods evaluate feature subsets by training and validating a machine learning model with those features. They assess the performance of the model to select the best subset of features.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "Dependence on Model: Feature subsets are evaluated based on their impact on model performance.\n",
    "Computational Intensity: Often more computationally expensive due to repeated model training.\n",
    "Common Techniques:\n",
    "\n",
    "Forward Selection: Starts with no features and iteratively adds features that improve model performance.\n",
    "Backward Elimination: Starts with all features and iteratively removes features that least impact model performance.\n",
    "Recursive Feature Elimination (RFE): Iteratively removes the least important features based on model weights or feature importance.\n",
    "Advantages:\n",
    "\n",
    "Considers feature interactions and their impact on model performance.\n",
    "Often results in better feature subsets tailored to the specific model.\n",
    "Disadvantages:\n",
    "\n",
    "Computationally expensive, especially with large feature sets.\n",
    "May overfit the model to the training data.\n",
    "3. Embedded Methods\n",
    "Definition: Embedded methods perform feature selection as part of the model training process. They incorporate feature selection within the model training procedure, using the model's internal mechanisms to identify important features.\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "Integration: Feature selection is integrated with model training.\n",
    "Efficiency: More efficient than wrapper methods as they avoid separate feature selection steps.\n",
    "Common Techniques:\n",
    "\n",
    "Lasso Regression (L1 Regularization): Adds a penalty to the loss function for large coefficients, shrinking some feature coefficients to zero, effectively performing feature selection.\n",
    "Decision Trees and Random Forests: Use tree-based models to evaluate feature importance. Features that contribute more to reducing impurity in the trees are considered more important.\n",
    "Advantages:\n",
    "\n",
    "Often more efficient than wrapper methods.\n",
    "Directly tied to the performance of the model being trained.\n",
    "Disadvantages:\n",
    "\n",
    "Feature selection is specific to the chosen model and may not generalize across different models.\n",
    "May not work well if the model does not inherently support feature importance measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf92543d-11ba-4d09-b5db-0e37dac58ad6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d668165-a9bd-4b12-bcf1-715d245c8fca",
   "metadata": {},
   "source": [
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64878c35-ed9a-462c-acd7-9887a8afb896",
   "metadata": {},
   "source": [
    "A3. Feature Selection is crucial in machine learning for improving model performance and efficiency. The Filter and Wrapper approaches are two common methods used to select features. Here's a detailed description of each, along with their pros and cons:\n",
    "\n",
    "1. Filter Approach\n",
    "Definition: The Filter approach evaluates the relevance of features based on statistical measures or metrics independently of any machine learning model. It involves ranking features according to their statistical properties and selecting the top-ranked ones.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "Compute Statistics: Calculate statistical measures or scores for each feature in relation to the target variable.\n",
    "Ranking: Rank features based on these scores.\n",
    "Selection: Select features based on predefined thresholds or the top-ranked features.\n",
    "Common Techniques:\n",
    "\n",
    "Correlation Coefficient: Measures the linear relationship between features and the target variable.\n",
    "Chi-Square Test: Assesses the independence between categorical features and the target.\n",
    "Mutual Information: Measures the amount of information gained about the target variable from each feature.\n",
    "Pros:\n",
    "\n",
    "Efficiency: Fast and computationally efficient as it does not involve training a model.\n",
    "Simplicity: Easy to implement and understand.\n",
    "Independence: Does not rely on a specific machine learning model, making it applicable across different models.\n",
    "Cons:\n",
    "\n",
    "Ignoring Interactions: Does not consider interactions between features or between features and the target variable.\n",
    "Potential Oversight: May miss important features that are relevant in combination with other features but not individually.\n",
    "Limited Scope: Only evaluates features based on statistical properties, which might not capture the full impact on model performance.\n",
    "2. Wrapper Approach\n",
    "Definition: The Wrapper approach evaluates feature subsets by training and validating a machine learning model using those features. It assesses the performance of the model to select the best subset of features.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "Feature Subsets: Generate different subsets of features.\n",
    "Model Training: Train a model using each feature subset.\n",
    "Evaluation: Evaluate the model’s performance using metrics such as accuracy, precision, recall, or any other relevant measure.\n",
    "Selection: Choose the subset of features that results in the best model performance.\n",
    "Common Techniques:\n",
    "\n",
    "Forward Selection: Starts with no features and iteratively adds features that improve model performance.\n",
    "Backward Elimination: Starts with all features and iteratively removes features that least impact model performance.\n",
    "Recursive Feature Elimination (RFE): Uses model-specific importance scores to iteratively remove the least important features.\n",
    "Pros:\n",
    "\n",
    "Consideration of Feature Interactions: Evaluates the impact of feature subsets on model performance, capturing interactions between features.\n",
    "Model-Specific: Tailored to the specific machine learning model being used, potentially leading to better feature subsets for that model.\n",
    "Improved Accuracy: Often results in better performance as it directly optimizes feature subsets for the model.\n",
    "Cons:\n",
    "\n",
    "Computationally Intensive: Requires multiple iterations of model training and evaluation, which can be computationally expensive and time-consuming.\n",
    "Risk of Overfitting: Especially with small datasets or too many feature subsets, there is a risk of overfitting to the training data.\n",
    "Complexity: More complex to implement compared to filter methods, and the results are dependent on the model used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4942208-1ef2-4ce1-a0a2-91aef260cee0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a007434d-8c29-4943-b3fe-830b7e0abf79",
   "metadata": {},
   "source": [
    "4.\r\n",
    "\r\n",
    "i. Describe the overall feature selection process.\r\n",
    "\r\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0de075-fe07-49e4-be6a-bd546176ea3e",
   "metadata": {},
   "source": [
    "A4. i. The Overall Feature Selection Process\n",
    "Feature Selection is a critical step in the data preprocessing phase of a machine learning project. The goal is to improve model performance by identifying and selecting the most relevant features while removing irrelevant or redundant ones. Here is an overview of the feature selection process:\n",
    "\n",
    "Define the Objective:\n",
    "\n",
    "Determine Goals: Understand the problem domain and what you aim to achieve with feature selection (e.g., improve model performance, reduce computational cost).\n",
    "Understand the Data:\n",
    "\n",
    "Data Exploration: Perform exploratory data analysis (EDA) to understand the structure, distribution, and relationships in the data. Identify potential features and their types (e.g., numerical, categorical).\n",
    "Choose the Feature Selection Method:\n",
    "\n",
    "Select Approach: Decide on a feature selection method based on the dataset size, model type, and computational resources. The main methods include filter, wrapper, and embedded approaches.\n",
    "Apply Feature Selection:\n",
    "\n",
    "Filter Methods: Compute statistical measures or metrics (e.g., correlation, chi-square) to rank features based on their relevance to the target variable.\n",
    "Wrapper Methods: Generate feature subsets, train a model using these subsets, and evaluate model performance to select the best subset.\n",
    "Embedded Methods: Integrate feature selection within the model training process, using techniques like Lasso regression or tree-based feature importance.\n",
    "Evaluate Feature Subsets:\n",
    "\n",
    "Assess Performance: Evaluate the impact of selected features on model performance using metrics such as accuracy, precision, recall, or F1 score.\n",
    "Validate Results: Ensure that the selected features improve model performance on validation or test data.\n",
    "Refine and Iterate:\n",
    "\n",
    "Adjust Feature Set: Based on performance evaluations, refine the feature set by adding, removing, or modifying features.\n",
    "Iterate: Repeat the feature selection process as needed to optimize the feature set.\n",
    "Finalize and Use:\n",
    "\n",
    "Finalize Features: Once the optimal feature set is determined, finalize the selection and use it for model training and evaluation.\n",
    "Deploy Model: Train the final model using the selected features and deploy it for predictions.\n",
    "ii. Feature Extraction: Key Underlying Principle and Algorithms\n",
    "Feature Extraction involves transforming raw data into a set of new features that can better represent the underlying patterns and relationships in the data. The goal is to reduce the dimensionality of the data while preserving important information.\n",
    "\n",
    "Key Underlying Principle:\n",
    "\n",
    "Dimensionality Reduction: Feature extraction reduces the number of features in a dataset by transforming the original features into a lower-dimensional space. The transformed features (or components) capture the most significant information and patterns, allowing for more efficient modeling and analysis.\n",
    "Example:\n",
    "\n",
    "Principal Component Analysis (PCA): PCA is a common feature extraction technique that transforms the data into a set of orthogonal components (principal components) ordered by the amount of variance they capture. The first few principal components often capture most of the variance in the data, allowing for dimensionality reduction while retaining essential information.\n",
    "Widely Used Feature Extraction Algorithms:\n",
    "\n",
    "Principal Component Analysis (PCA):\n",
    "\n",
    "Description: PCA transforms the data into a set of linearly uncorrelated components that capture the maximum variance in the data.\n",
    "Application: Used for reducing dimensionality while preserving variance, often in image processing and data visualization.\n",
    "Linear Discriminant Analysis (LDA):\n",
    "\n",
    "Description: LDA projects data onto a lower-dimensional space that maximizes class separability. Unlike PCA, which focuses on variance, LDA focuses on maximizing the separation between different classes.\n",
    "Application: Used in classification problems to enhance class separation.\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE):\n",
    "\n",
    "Description: t-SNE is a non-linear dimensionality reduction technique that projects high-dimensional data into a lower-dimensional space while preserving local structure and distances between data points.\n",
    "Application: Commonly used for visualizing complex, high-dimensional data.\n",
    "Singular Value Decomposition (SVD):\n",
    "\n",
    "Description: SVD decomposes a matrix into three matrices (U, Σ, V) that represent the original matrix in terms of singular values and vectors. It is used for dimensionality reduction and feature extraction.\n",
    "Application: Widely used in text processing (e.g., Latent Semantic Analysis) and recommendation systems.\n",
    "Independent Component Analysis (ICA):\n",
    "\n",
    "Description: ICA aims to find statistically independent components from mixed signals. It is used to separate mixed signals into their original sources.\n",
    "Application: Used in signal processing and image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daebcbf-239f-4326-8caa-1824a5eb919c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44402120-25cd-45f3-84cb-c1746e70f6b1",
   "metadata": {},
   "source": [
    "5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9e596-bb80-4701-961d-59102f00804c",
   "metadata": {},
   "source": [
    "A5. Feature engineering is a critical step in text categorization (or text classification) that involves creating and transforming features from raw text data to improve the performance of classification models. Here’s a detailed description of the feature engineering process in the context of text categorization:\n",
    "\n",
    "Feature Engineering Process for Text Categorization\n",
    "Understand the Problem and Data:\n",
    "\n",
    "Define Objective: Understand the text categorization task (e.g., spam detection, sentiment analysis, topic classification).\n",
    "Data Collection: Gather a corpus of text documents and their corresponding labels (categories).\n",
    "Text Preprocessing:\n",
    "\n",
    "Tokenization: Break down text into individual words or tokens. This can be done at the word level, sentence level, or character level.\n",
    "Lowercasing: Convert all text to lowercase to ensure uniformity and avoid treating the same word with different cases as different tokens.\n",
    "Stopword Removal: Remove common words (e.g., \"and\", \"the\") that do not contribute significant meaning or value to the text classification.\n",
    "Punctuation Removal: Remove punctuation marks that are not relevant to the categorization.\n",
    "Stemming/Lemmatization: Reduce words to their base or root forms (e.g., \"running\" to \"run\") to standardize tokens.\n",
    "Feature Extraction:\n",
    "\n",
    "Bag of Words (BoW): Represent text as a vector of word frequencies or counts. Each unique word in the corpus becomes a feature.\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF): Calculate the importance of each word based on its frequency in a document relative to its frequency across the entire corpus. This helps to weigh terms that are more informative.\n",
    "N-grams: Extract sequences of n words (e.g., bigrams, trigrams) to capture word combinations and contextual information.\n",
    "Word Embeddings: Use pre-trained word embeddings (e.g., Word2Vec, GloVe) or contextual embeddings (e.g., BERT) to represent words as dense vectors that capture semantic meaning and relationships.\n",
    "Feature Transformation:\n",
    "\n",
    "Dimensionality Reduction: Apply techniques such as PCA or Truncated SVD to reduce the number of features while retaining important information. This is useful when dealing with high-dimensional feature spaces like BoW or TF-IDF.\n",
    "Feature Scaling: Normalize or standardize feature values if needed, especially for models that are sensitive to feature magnitudes.\n",
    "Feature Selection:\n",
    "\n",
    "Statistical Tests: Use statistical tests to select features that have a significant impact on the categorization task (e.g., chi-square test, mutual information).\n",
    "Feature Importance: Evaluate feature importance using methods like chi-square test, or model-based methods like tree-based feature importance.\n",
    "Create Additional Features:\n",
    "\n",
    "Text Length: Include features such as the length of the text or the number of words/sentences.\n",
    "Sentiment Scores: Extract sentiment scores or other metadata (e.g., keyword presence) if relevant to the categorization task.\n",
    "Named Entities: Identify and extract named entities (e.g., names, dates, locations) which might be important for certain text categorization problems.\n",
    "Combine Features:\n",
    "\n",
    "Feature Engineering: Combine different types of features (e.g., TF-IDF with word embeddings) to create a comprehensive feature set that captures various aspects of the text.\n",
    "Feature Interaction: Create features that capture interactions between different types of features if applicable.\n",
    "Model Training and Evaluation:\n",
    "\n",
    "Train Model: Use the engineered features to train a machine learning model (e.g., logistic regression, support vector machines, neural networks).\n",
    "Evaluate: Assess model performance using metrics such as accuracy, precision, recall, F1 score, and confusion matrix.\n",
    "Iterate and Refine:\n",
    "\n",
    "Adjust Features: Based on model performance, refine and adjust the feature set by adding, removing, or modifying features.\n",
    "Optimize: Experiment with different feature engineering techniques and combinations to optimize model performance.\n",
    "Example\n",
    "Consider a text categorization problem where the task is to classify news articles into different categories such as sports, politics, and entertainment.\n",
    "\n",
    "Preprocessing: Tokenize the text, remove stopwords and punctuation, convert to lowercase, and perform stemming.\n",
    "Feature Extraction:\n",
    "BoW: Convert articles into feature vectors representing word frequencies.\n",
    "TF-IDF: Compute TF-IDF scores for each word to capture important terms.\n",
    "N-grams: Extract bigrams (e.g., \"economic growth\") to capture common phrases.\n",
    "Word Embeddings: Use pre-trained embeddings to represent words in a dense vector space.\n",
    "Feature Transformation: Apply Truncated SVD to reduce dimensionality of the TF-IDF features.\n",
    "Feature Selection: Use statistical tests to select the most informative features.\n",
    "Create Additional Features: Include the length of the article and sentiment scores if relevant.\n",
    "Combine Features: Combine TF-IDF features with word embeddings and sentiment scores.\n",
    "Train and Evaluate: Train a classifier using the combined features and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33872ed8-9676-4b91-9564-e6826598f12a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e36e189-7c69-4f46-9a9f-731553614126",
   "metadata": {},
   "source": [
    "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995dd35d-60a2-4e2c-85ba-68c5db6c628c",
   "metadata": {},
   "source": [
    "A6. Cosine Similarity is a popular metric for measuring the similarity between two text documents in text categorization. It is particularly well-suited for text analysis due to several key properties:\r\n",
    "Why Cosine Similarity is Good for Text Categorization\r\n",
    "1.\tMagnitude Invariance:\r\n",
    "o\tNormalization: Cosine similarity measures the cosine of the angle between two vectors, ignoring the magnitude. This makes it effective for comparing text documents of different lengths because it focuses on the direction of the vectors rather than their size.\r\n",
    "2.\tText Representation:\r\n",
    "o\tHigh Dimensionality: Text data is often represented in high-dimensional spaces (e.g., word frequencies in a document-term matrix). Cosine similarity works well in these high-dimensional spaces, as it measures the similarity based on the distribution of terms rather than their absolute counts.\r\n",
    "3.\tSimilarity Between Documents:\r\n",
    "o\tSemantic Similarity: Cosine similarity captures how similar two documents are in terms of their content, irrespective of the length. Documents with similar content will have vectors that point in similar directions, resulting in a high cosine similarity score.\r\n",
    "4.\tEffective for Sparse Data:\r\n",
    "o\tSparsity: Text data is usually sparse (many terms have zero counts). Cosine similarity handles sparsity well, as it only considers the non-zero components of the vectors.\r\n",
    "Cosine Similarity Calculation\r\n",
    "To find the cosine similarity between two vectors, you use the following formula:\r\n",
    "Cosine Similarity=A⋅B∥A∥∥B∥\\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}Cosine Similarity=∥A∥∥B∥A⋅B\r\n",
    "where:\r\n",
    "•\tA\\mathbf{A}A and B\\mathbf{B}B are the vectors representing the two documents.\r\n",
    "•\tA⋅B\\mathbf{A} \\cdot \\mathbf{B}A⋅B is the dot product of the vectors.\r\n",
    "•\t∥A∥\\|\\mathbf{A}\\|∥A∥ and ∥B∥\\|\\mathbf{B}\\|∥B∥ are the magnitudes (Euclidean norms) of the vectors.\r\n",
    "Given the vectors:\r\n",
    "•\tA=(2,3,2,0,2,3,3,0,1)\\mathbf{A} = (2, 3, 2, 0, 2, 3, 3, 0, 1)A=(2,3,2,0,2,3,3,0,1)\r\n",
    "•\tB=(2,1,0,0,3,2,1,3,1)\\mathbf{B} = (2, 1, 0, 0, 3, 2, 1, 3, 1)B=(2,1,0,0,3,2,1,3,1)\r\n",
    "Step-by-Step Calculation\r\n",
    "1.\tDot Product:\r\n",
    "A⋅B=(2⋅2)+(3⋅1)+(2⋅0)+(0⋅0)+(2⋅3)+(3⋅2)+(3⋅1)+(0⋅3)+(1⋅1)\\mathbf{A} \\cdot \\mathbf{B} = (2 \\cdot 2) + (3 \\cdot 1) + (2 \\cdot 0) + (0 \\cdot 0) + (2 \\cdot 3) + (3 \\cdot 2) + (3 \\cdot 1) + (0 \\cdot 3) + (1 \\cdot 1)A⋅B=(2⋅2)+(3⋅1)+(2⋅0)+(0⋅0)+(2⋅3)+(3⋅2)+(3⋅1)+(0⋅3)+(1⋅1) A⋅B=4+3+0+0+6+6+3+0+1=23\\mathbf{A} \\cdot \\mathbf{B} = 4 + 3 + 0 + 0 + 6 + 6 + 3 + 0 + 1 = 23A⋅B=4+3+0+0+6+6+3+0+1=23\r\n",
    "2.\tMagnitudes:\r\n",
    "o\tFor vector A\\mathbf{A}A:\r\n",
    "∥A∥=(22)+(32)+(22)+(02)+(22)+(32)+(32)+(02)+(12)\\|\\mathbf{A}\\| = \\sqrt{(2^2) + (3^2) + (2^2) + (0^2) + (2^2) + (3^2) + (3^2) + (0^2) + (1^2)}∥A∥=(22)+(32)+(22)+(02)+(22)+(32)+(32)+(02)+(12) ∥A∥=4+9+4+0+4+9+9+0+1\\|\\mathbf{A}\\| = \\sqrt{4 + 9 + 4 + 0 + 4 + 9 + 9 + 0 + 1}∥A∥=4+9+4+0+4+9+9+0+1 ∥A∥=40≈6.32\\|\\mathbf{A}\\| = \\sqrt{40} \\approx 6.32∥A∥=40≈6.32\r\n",
    "o\tFor vector B\\mathbf{B}B:\r\n",
    "∥B∥=(22)+(12)+(02)+(02)+(32)+(22)+(12)+(32)+(12)\\|\\mathbf{B}\\| = \\sqrt{(2^2) + (1^2) + (0^2) + (0^2) + (3^2) + (2^2) + (1^2) + (3^2) + (1^2)}∥B∥=(22)+(12)+(02)+(02)+(32)+(22)+(12)+(32)+(12) ∥B∥=4+1+0+0+9+4+1+9+1\\|\\mathbf{B}\\| = \\sqrt{4 + 1 + 0 + 0 + 9 + 4 + 1 + 9 + 1}∥B∥=4+1+0+0+9+4+1+9+1 ∥B∥=29≈5.39\\|\\mathbf{B}\\| = \\sqrt{29} \\approx 5.39∥B∥=29≈5.39\r\n",
    "3.\tCosine Similarity:\r\n",
    "Cosine Similarity=236.32×5.39\\text{Cosine Similarity} = \\frac{23}{6.32 \\times 5.39}Cosine Similarity=6.32×5.3923 Cosine Similarity=2334.04≈0.676\\text{Cosine Similarity} = \\frac{23}{34.04} \\approx 0.676Cosine Similarity=34.0423≈0.676\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392acd3-9942-44be-8868-249d87afe7c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "745483d4-fc3d-46e0-8b55-38961ab0073d",
   "metadata": {},
   "source": [
    "7.\r\n",
    "\r\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\r\n",
    "\r\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\r\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229244f-d425-44a8-ba5e-eeef3b16347f",
   "metadata": {},
   "source": [
    "A7. i. Hamming Distance\r\n",
    "Definition: The Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols differ. It is commonly used to measure the similarity between two binary strings or sequences.\r\n",
    "Formula: Hamming Distance=∑i=1nδ(xi,yi)\\text{Hamming Distance} = \\sum_{i=1}^n \\delta(x_i, y_i)Hamming Distance=∑i=1nδ(xi,yi) where:\r\n",
    "•\txix_ixi and yiy_iyi are the symbols at position iii in the two strings.\r\n",
    "•\tδ(xi,yi)\\delta(x_i, y_i)δ(xi,yi) is 1 if xi≠yix_i \\neq y_ixi=yi, and 0 if xi=yix_i = y_ixi=yi.\r\n",
    "Calculation:\r\n",
    "Given strings: 10001011 and 11001111.\r\n",
    "1.\tCompare each corresponding bit:\r\n",
    "Copy code\r\n",
    "1 0 0 0 1 0 1 1\r\n",
    "1 1 0 0 1 1 1 1\r\n",
    "2.\tIdentify the positions where the bits differ:\r\n",
    "o\tPosition 1: 1 vs. 1 (No difference)\r\n",
    "o\tPosition 2: 0 vs. 1 (Difference)\r\n",
    "o\tPosition 3: 0 vs. 0 (No difference)\r\n",
    "o\tPosition 4: 0 vs. 0 (No difference)\r\n",
    "o\tPosition 5: 1 vs. 1 (No difference)\r\n",
    "o\tPosition 6: 0 vs. 1 (Difference)\r\n",
    "o\tPosition 7: 1 vs. 1 (No difference)\r\n",
    "o\tPosition 8: 1 vs. 1 (No difference)\r\n",
    "Differences occur at positions 2 and 6.\r\n",
    "3.\tCount the number of differences:\r\n",
    "Hamming Distance=2\\text{Hamming Distance} = 2Hamming Distance=2\r\n",
    "ii. Jaccard Index and Similarity Matching Coefficient\r\n",
    "Definitions:\r\n",
    "•\tJaccard Index: Measures the similarity between two sets by dividing the size of their intersection by the size of their union.\r\n",
    "Jaccard Index=∣A∩B∣∣A∪B∣\\text{Jaccard Index} = \\frac{|A \\cap B|}{|A \\cup B|}Jaccard Index=∣A∪B∣∣A∩B∣\r\n",
    "where AAA and BBB are sets of features.\r\n",
    "•\tSimilarity Matching Coefficient: Measures the proportion of matching elements between two binary vectors. It is similar to the Jaccard Index but often used in different contexts.\r\n",
    "Similarity Matching Coefficient=Number of MatchesTotal Number of Elements\\text{Similarity Matching Coefficient} = \\frac{\\text{Number of Matches}}{\\text{Total Number of Elements}}Similarity Matching Coefficient=Total Number of ElementsNumber of Matches\r\n",
    "Calculation:\r\n",
    "Vectors:\r\n",
    "•\tFor Jaccard Index: (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1)\r\n",
    "•\tFor Similarity Matching Coefficient: (1, 0, 0, 1, 1, 0, 0, 1) and (1, 0, 0, 1, 1, 0, 0, 1)\r\n",
    "1.\tJaccard Index Calculation:\r\n",
    "o\tIntersection: Positions with 1 in both vectors are at indices 1, 2, 3, and 4. Thus, A∩B={1,1,0,0,1,0,1,1}∩{1,1,0,0,0,1,1,1}A \\cap B = \\{1, 1, 0, 0, 1, 0, 1, 1\\} \\cap \\{1, 1, 0, 0, 0, 1, 1, 1\\}A∩B={1,1,0,0,1,0,1,1}∩{1,1,0,0,0,1,1,1}.\r\n",
    "o\tIntersection vector: (1, 1, 0, 0, 0, 0, 1, 1)\r\n",
    "o\tSize of Intersection = 5\r\n",
    "o\tUnion: All unique positions with at least one 1 in either vector are: (1, 1, 0, 0, 1, 1, 1, 1)\r\n",
    "o\tSize of Union = 7\r\n",
    "o\tJaccard Index: Jaccard Index=57≈0.714\\text{Jaccard Index} = \\frac{5}{7} \\approx 0.714Jaccard Index=75≈0.714\r\n",
    "2.\tSimilarity Matching Coefficient Calculation:\r\n",
    "o\tMatches: Positions where both vectors are 1 are at indices 1, 4, 5, and 8.\r\n",
    "o\tNumber of Matches = 4\r\n",
    "o\tTotal Elements: 8\r\n",
    "o\tSimilarity Matching Coefficient: Similarity Matching Coefficient=48=0.5\\text{Similarity Matching Coefficient} = \\frac{4}{8} = 0.5Similarity Matching Coefficient=84=0.5\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0c542-c991-4605-a34a-8b355debfc72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bef2e2a7-d76d-456d-bde8-4b57781b1607",
   "metadata": {},
   "source": [
    "8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e60c8-329a-4027-8086-ee95b7284914",
   "metadata": {},
   "source": [
    "A8. High-Dimensional Data Set: A data set is considered high-dimensional when it contains a large number of features (or dimensions) relative to the number of observations (or samples). In other words, each data point is described by a large number of attributes or variables.\n",
    "\n",
    "Real-Life Examples of High-Dimensional Data Sets\n",
    "Text Data:\n",
    "\n",
    "Term Document Matrix: In natural language processing (NLP), documents are often represented as vectors where each dimension corresponds to a unique word in the corpus. This can result in thousands to millions of dimensions, especially with large vocabularies.\n",
    "Genomics:\n",
    "\n",
    "Gene Expression Data: In genomics, each gene represents a feature, and experiments may involve measuring the expression levels of thousands of genes across a few samples.\n",
    "Image Data:\n",
    "\n",
    "Pixel Data: Each image in a dataset is represented as a grid of pixels. For example, a 256x256 grayscale image has 65,536 dimensions (one for each pixel).\n",
    "Financial Data:\n",
    "\n",
    "Stock Market Data: Features might include historical prices, trading volumes, technical indicators, and other metrics across many time periods, creating a high-dimensional feature space.\n",
    "Medical Data:\n",
    "\n",
    "Patient Records: In healthcare, each patient’s record might include hundreds of features such as lab results, medical history, and demographic information.\n",
    "Difficulties with High-Dimensional Data\n",
    "Curse of Dimensionality:\n",
    "\n",
    "Sparse Data: As the number of dimensions increases, data becomes sparse, making it harder to find meaningful patterns.\n",
    "Distance Measures: In high dimensions, distances between data points become less informative and less discriminative, which can affect distance-based algorithms.\n",
    "Overfitting:\n",
    "\n",
    "Model Complexity: High-dimensional data often leads to complex models that can overfit the training data, capturing noise rather than the underlying pattern.\n",
    "Computational Cost:\n",
    "\n",
    "Resource Intensive: High-dimensional data increases computational cost for training and inference, requiring more memory and processing power.\n",
    "Feature Redundancy:\n",
    "\n",
    "Correlated Features: Many features may be redundant or highly correlated, which can make feature selection and model training more challenging.\n",
    "Strategies to Address High-Dimensional Data Challenges\n",
    "Dimensionality Reduction:\n",
    "\n",
    "Principal Component Analysis (PCA): Reduces dimensionality by projecting data onto the directions of maximum variance.\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE): Useful for visualizing high-dimensional data in two or three dimensions while preserving local structures.\n",
    "Linear Discriminant Analysis (LDA): Reduces dimensions while maximizing class separability.\n",
    "Feature Selection:\n",
    "\n",
    "Statistical Methods: Techniques like chi-square tests, mutual information, or ANOVA can identify and retain the most relevant features.\n",
    "Regularization: Methods like L1 (Lasso) regularization can help in feature selection by penalizing irrelevant features.\n",
    "Feature Engineering:\n",
    "\n",
    "Combine Features: Create new features that capture essential patterns from the original features.\n",
    "Domain Knowledge: Use knowledge about the data to manually select or engineer features that are most relevant.\n",
    "Model Selection and Evaluation:\n",
    "\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to ensure the model generalizes well and does not overfit.\n",
    "Ensemble Methods: Techniques like random forests or gradient boosting can handle high-dimensional data by aggregating the results of multiple models.\n",
    "Data Augmentation:\n",
    "\n",
    "Synthetic Data: Create additional samples through techniques like SMOTE (Synthetic Minority Over-sampling Technique) to enrich the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5480e57-c460-41f7-8f1c-84051e3fa1d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce1edf6-3763-40b7-a8cc-2cbec6490b70",
   "metadata": {},
   "source": [
    "9. Make a few quick notes on:\r\n",
    "\r\n",
    "PCA is an acronym for Personal Computer Analysis.\r\n",
    "\r\n",
    "2. Use of vectors\r\n",
    "\r\n",
    "3. Embedded technique\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98de096-cab2-4d03-b4ad-470c89ac9461",
   "metadata": {},
   "source": [
    "A9. \n",
    "\n",
    "PCA (Principal Component Analysis):\n",
    "\n",
    "Definition: PCA is a dimensionality reduction technique used to identify the principal components of a dataset, which are the directions of maximum variance. It helps reduce the number of features while retaining most of the information.\n",
    "Misconception: PCA stands for Principal Component Analysis, not Personal Computer Analysis.\n",
    "Use of Vectors:\n",
    "\n",
    "Definition: Vectors are mathematical entities with both magnitude and direction. In machine learning and data analysis, vectors are used to represent data points, features, and weights in various algorithms.\n",
    "Applications:\n",
    "Feature Representation: In text analysis, words are represented as vectors in vector space models.\n",
    "Distance Calculation: Used to measure similarity or distance between data points (e.g., Euclidean distance).\n",
    "Data Transformation: In dimensionality reduction techniques like PCA, vectors represent principal components.\n",
    "Embedded Technique:\n",
    "\n",
    "Definition: Embedded techniques refer to methods where feature selection or dimensionality reduction is incorporated into the model training process. They are not separate from model training but integrated into it.\n",
    "Examples:\n",
    "Regularization: Techniques like L1 regularization (Lasso) that add penalties to the loss function based on feature weights, effectively performing feature selection.\n",
    "Tree-based Methods: Algorithms like Random Forests or Gradient Boosting can rank features based on their importance, automatically selecting relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f7e1a-3a97-4c70-ba83-281b6899f2f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86caea83-1cac-422d-ba7c-a659feab42e3",
   "metadata": {},
   "source": [
    "10. Make a comparison between:\r\n",
    "\r\n",
    "1. Sequential backward exclusion vs. sequential forward selection\r\n",
    "\r\n",
    "2. Function selection methods: filter vs. wrapper\r\n",
    "\r\n",
    "3. SMC vs. Jaccard coefficient\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477fdde6-f970-419c-82e6-d9ef9695873c",
   "metadata": {},
   "source": [
    "A10. Here’s a comparison of the specified techniques and metrics:\r\n",
    "1. Sequential Backward Exclusion vs. Sequential Forward Selection\r\n",
    "•\tSequential Backward Exclusion:\r\n",
    "o\tDefinition: A feature selection method that starts with all features and iteratively removes the least significant feature based on a predefined criterion (e.g., model performance).\r\n",
    "o\tProcess:\r\n",
    "1.\tStart with the full feature set.\r\n",
    "2.\tRemove one feature at a time.\r\n",
    "3.\tEvaluate the model performance after each removal.\r\n",
    "4.\tContinue until no further improvement is observed or a stopping criterion is met.\r\n",
    "o\tPros:\r\n",
    "\tOften finds a subset of features that maximizes performance.\r\n",
    "\tCan be useful when the number of features is relatively small.\r\n",
    "o\tCons:\r\n",
    "\tComputationally expensive with a large number of features.\r\n",
    "\tMay not find the globally optimal subset.\r\n",
    "•\tSequential Forward Selection:\r\n",
    "o\tDefinition: A feature selection method that starts with an empty set of features and iteratively adds the most significant feature based on a predefined criterion.\r\n",
    "o\tProcess:\r\n",
    "1.\tStart with an empty feature set.\r\n",
    "2.\tAdd one feature at a time.\r\n",
    "3.\tEvaluate the model performance after each addition.\r\n",
    "4.\tContinue until adding more features does not improve performance or a stopping criterion is met.\r\n",
    "o\tPros:\r\n",
    "\tCan improve model performance incrementally.\r\n",
    "\tUseful when starting with no prior knowledge of feature importance.\r\n",
    "o\tCons:\r\n",
    "\tCan be computationally intensive if the feature set is large.\r\n",
    "\tMay not explore all possible combinations, potentially missing the optimal set.\r\n",
    "2. Feature Selection Methods: Filter vs. Wrapper\r\n",
    "•\tFilter Methods:\r\n",
    "o\tDefinition: Feature selection techniques that evaluate features independently of any machine learning algorithm. Features are ranked based on statistical tests or criteria and selected based on these rankings.\r\n",
    "o\tExamples: Chi-square test, mutual information, ANOVA.\r\n",
    "o\tPros:\r\n",
    "\tComputationally less expensive since it doesn’t involve model training.\r\n",
    "\tScalable to high-dimensional data.\r\n",
    "o\tCons:\r\n",
    "\tMay not capture interactions between features.\r\n",
    "\tLacks consideration of feature dependencies.\r\n",
    "•\tWrapper Methods:\r\n",
    "o\tDefinition: Feature selection techniques that evaluate subsets of features by training a machine learning model on them and using the model’s performance to guide feature selection.\r\n",
    "o\tExamples: Sequential forward selection, sequential backward exclusion, recursive feature elimination (RFE).\r\n",
    "o\tPros:\r\n",
    "\tConsiders feature interactions and dependencies.\r\n",
    "\tTypically results in better performance for the chosen model.\r\n",
    "o\tCons:\r\n",
    "\tComputationally expensive as it involves training multiple models.\r\n",
    "\tCan be impractical with a very large number of features due to the high computational cost.\r\n",
    "3. SMC (Similarity Matching Coefficient) vs. Jaccard Coefficient\r\n",
    "•\tSimilarity Matching Coefficient (SMC):\r\n",
    "o\tDefinition: Measures the proportion of matching elements between two binary vectors. It is a simple metric that counts the number of identical positions and divides it by the total number of elements.\r\n",
    "o\tFormula: SMC=Number of Matching ElementsTotal Number of Elements\\text{SMC} = \\frac{\\text{Number of Matching Elements}}{\\text{Total Number of Elements}}SMC=Total Number of ElementsNumber of Matching Elements\r\n",
    "o\tUse Case: Commonly used for comparing binary features or attributes.\r\n",
    "•\tJaccard Coefficient:\r\n",
    "o\tDefinition: Measures the similarity between two sets by dividing the size of their intersection by the size of their union. It is used for comparing sets rather than vectors.\r\n",
    "o\tFormula: Jaccard Coefficient=∣A∩B∣∣A∪B∣\\text{Jaccard Coefficient} = \\frac{|A \\cap B|}{|A \\cup B|}Jaccard Coefficient=∣A∪B∣∣A∩B∣\r\n",
    "o\tUse Case: Often used for comparing sets or features where overlap is important, such as in text analysis or clustering.\r\n",
    "Comparison:\r\n",
    "•\tSMC focuses on matching elements within the same size binary vectors, while the Jaccard Coefficient focuses on the proportion of overlap relative to the total size of the union of two sets.\r\n",
    "•\tSMC is simpler and applicable to binary vectors where the total number of elements is constant and comparable. Jaccard Coefficient is more versatile for comparing sets with varying sizes and is often used in contexts where the sets are not necessarily of equal length.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185cf90-523f-42f3-93c9-4e62e10b8e53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cee585e-44d4-484c-98c6-e4b554912da1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f912082d-1fb5-47bf-8fc0-9e03482699ca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
